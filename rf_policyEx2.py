import numpy as np
import time

def show_v_table(v_table, env):
    for i in range(env.reward.shape[0]):
        print("+-----------------"*env.reward.shape[1],end="")
        print("+")
        for k in range(3):
            print("|",end="")
            for j in range(env.reward.shape[1]):
                if k==0:
                    print("                 |",end="")
                if k==1:
                        print("   {0:8.2f}      |".format(v_table[i,j]),end="")
                if k==2:
                    print("                 |",end="")
            print()
    print("+-----------------"*env.reward.shape[1],end="")
    print("+")



# ì •ì±… policy í™”ì‚´í‘œë¡œ ê·¸ë¦¬ê¸°
def show_policy(policy,env):
    for i in range(env.reward.shape[0]):
        print("+-----------------"*env.reward.shape[1],end="")
        print("+")
        for k in range(3):
            print("|",end="")
            for j in range(env.reward.shape[1]):
                if k==0:
                    print("                 |",end="")
                if k==1:
                    if policy[i,j] == 0:
                        print("      â†‘         |",end="")
                    elif policy[i,j] == 1:
                        print("      â†’         |",end="")
                    elif policy[i,j] == 2:
                        print("      â†“         |",end="")
                    elif policy[i,j] == 3:
                        print("      â†         |",end="")
                if k==2:
                    print("                 |",end="")
            print()
    print("+-----------------"*env.reward.shape[1],end="")
    print("+")



class Agent():
    # í–‰ë™ì— ë”°ë¥¸ ì—ì´ì „íŠ¸ì˜ ì¢Œí‘œ ì´ë™(ìœ„, ì˜¤ë¥¸ìª½, ì•„ë˜, ì™¼ìª½)
    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])

    # ê° í–‰ë™ë³„ ì„ íƒí™•ë¥ 
    select_action_pr = np.array([0.25,0.25,0.25,0.25])

    # ì—ì´ì „íŠ¸ì˜ ì´ˆê¸° ìœ„ì¹˜ ì €ì¥
    def __init__(self):
        self.pos = (0,0)

    # ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ì €ì¥
    def set_pos(self, position):
        self.pos = position
        return self.pos

    # ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°
    def get_pos(self):
        return self.pos


class Environment():
    # ë¯¸ë¡œë°–(ì ˆë²½), ê¸¸, ëª©ì ì§€ì™€ ë³´ìƒ ì„¤ì •
    cliff = -3
    road = -1
    goal = 1

    # ëª©ì ì§€ ì¢Œí‘œ ì„¤ì •
    goal_position = [2,2]

    # ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ìˆ«ì
    reward_list = [[road,road,road],
                   [road,road,road],
                   [road,road,goal]]

    # ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ë¬¸ì
    reward_list1 = [["road","road","road"],
                    ["road","road","road"],
                    ["road","road","goal"]]

    # ë³´ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ arrayë¡œ ì„¤ì •
    def __init__(self):
        self.reward = np.asarray(self.reward_list)

    # ì„ íƒëœ ì—ì´ì „íŠ¸ì˜ í–‰ë™ ê²°ê³¼ ë°˜í™˜ (ë¯¸ë¡œë°–ì¼ ê²½ìš° ì´ì „ ì¢Œí‘œë¡œ ë‹¤ì‹œ ë³µê·€)
    def move(self, agent, action):

        done = False

        # í–‰ë™ì— ë”°ë¥¸ ì¢Œí‘œ êµ¬í•˜ê¸°
        new_pos = agent.pos + agent.action[action]

        # í˜„ì¬ì¢Œí‘œê°€ ëª©ì ì§€ ì¸ì§€í™•ì¸
        if self.reward_list1[agent.pos[0]][agent.pos[1]] == "goal":
            reward = self.goal
            observation = agent.set_pos(agent.pos)
            done = True
        # ì´ë™ í›„ ì¢Œí‘œê°€ ë¯¸ë¡œ ë°–ì¸ í™•ì¸
        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:
            reward = self.cliff
            observation = agent.set_pos(agent.pos)
            done = True
        # ì´ë™ í›„ ì¢Œí‘œê°€ ê¸¸ì´ë¼ë©´
        else:
            observation = agent.set_pos(new_pos)
            reward = self.reward[observation[0],observation[1]]

        return observation, reward, done #next state, done= arrive or cliff



# ë°˜ë³µ ì •ì±… í‰ê°€
np.random.seed(0)
env = Environment()
agent = Agent()
gamma = 0.9

# ğ‘‰(ğ‘ )=0ìœ¼ë¡œ ì´ˆê¸°í™”
v_table = np.zeros((env.reward.shape[0],env.reward.shape[1]))

print("start Iterative Value Evaluation")

k = 1
print()
print("V0(S)   k = 0")

# ì´ˆê¸°í™”ëœ V í…Œì´ë¸” ì¶œë ¥
show_v_table(np.round(v_table,2),env)

# ì‹œì‘ ì‹œê°„ ë³€ìˆ˜ì— ì €ì¥
start_time = time.time()


def policy_evalution(env, agent, v_table, policy):
    discount_factor = 0.9

    for i in range(env.reward.shape[0]):
        for j in range(env.reward.shape[1]):
            # ì—ì´ì „íŠ¸ë¥¼ ì§€ì •ëœ ì¢Œí‘œì— ìœ„ì¹˜ì‹œí‚¨í›„ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê³„ì‚°
            agent.set_pos([i,j])
            # í˜„ì¬ ì •ì±…ì˜ í–‰ë™ì„ ì„ íƒ
            action = policy[i,j]
            observation, reward, done = env.move(agent, action)
            v_table[i,j] = reward + discount_factor * v_table[observation[0],observation[1]]

    return v_table

def policy_improvement(env, agent, v_table, policy):

    discount_factor = 0.9

    for i in range(env.reward.shape[0]):
        for j in range(env.reward.shape[1]):
            # ê°€ëŠ¥í•œ í–‰ë™ì¤‘ ìµœëŒ“ê°’ì„ ê°€ì§€ëŠ” í–‰ë™ì„ ì„ íƒ
            temp_action = 0
            temp_value =  -1e+10
            for action in range(len(agent.action)):
                agent.set_pos([i,j])
                observation, reward, done = env.move(agent,action)
                if temp_value < reward + discount_factor * v_table[observation[0],observation[1]]:
                    temp_action = action
                    temp_value = reward + discount_factor * v_table[observation[0],observation[1]]

            policy[i,j] = temp_action
    return policy


def value_evaluation(env, agent, v_table, a_table):
    discount_factor = 0.9

    for i in range(env.reward.shape[0]):
        for j in range(env.reward.shape[1]):
            temp_action = 0
            temp_value = -1e+10

            for action in range(len(agent.action)):
                agent.set_pos([i, j])
                observation, reward, done = env.move(agent, action)
                if temp_value < reward + discount_factor * v_table[observation[0], observation[1]]:
                    temp_action = action
                    temp_value = reward + discount_factor * v_table[observation[0], observation[1]]

            a_table[i, j] = temp_action
            v_table[i, j] = temp_value

    return v_table, a_table

# def get_action(env, agent, v_table):


np.random.seed(0)
env = Environment()
agent = Agent()


v_table =  np.random.rand(env.reward.shape[0], env.reward.shape[1])
a_table =  np.random.rand(env.reward.shape[0], env.reward.shape[1])

policy = np.random.randint(0, 4,(env.reward.shape[0], env.reward.shape[1]))

print("Initial random V(S)")
show_v_table(np.round(v_table,2),env)
print()
print("Initial random Policy Ï€0(S)")
show_policy(policy,env)
print("start policy iteration")

# ì‹œì‘ ì‹œê°„ì„ ë³€ìˆ˜ì— ì €ì¥
start_time = time.time()

max_iter_number = 50
for iter_number in range(max_iter_number):

    #ì •ì±…í‰ê°€
    # v_table= policy_evalution(env, agent, v_table, policy)
    v_table, a_table = value_evaluation(env, agent, v_table, a_table)
    # ì •ì±… í‰ê°€ í›„ ê²°ê³¼ í‘œì‹œ
    print("")
    print("V:{0:}".format(iter_number))
    show_v_table(np.round(v_table,2),env)
    print()

    #ì •ì±…ê°œì„ 
    # policy = policy_improvement(env, agent, v_table, policy)

    # policy ë³€í™” ì €ì¥
    print("policy:{}".format(iter_number+1))
    show_policy(policy,env)


print("total_time = {}".format(time.time()-start_time))
